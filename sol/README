
Group Members : William Park, Ethan Kim, Nathan Luu
Subgroup 1 : William Park, Nathan Luu
Subgroup 2 : Ethan Kim

Our code uses machine learning that would produce a decision tree to summarize what we prefer.
For our tests, we used the vegetable examples. The decisions tree would have a node that does not have a leaf has
an attribute, the same attribute arises once, the node has edges, and there will be the predicated value for one
attribute. Our ListObjsData class would hold our dataset where we would have methods to use the dataset. We have
attributes to get the attributes (getAttribute), get the same value (allSameValue), get the size (size), split the
dataset into two subsets based on the attribute that we decided on, etc. This class would be used to produce the
dataset that would be used in the TreeGenerator class. The TreeGenerator class would generate a tree. The TreeGenerator
class would use a Node and Leaf classes in order to generate the tree.

1. The narrowing of the application pool shows bias as there will be a strong focus on the candidates who demonstrate
similar characteristics to previous hires. This is important to know because these algorithms can end up perpetrating
the historical bias companies are actively working to remedy. This means that candidates that don't have qualities that
resemble those of the historical hiring pool are not given a chance to even be considered by a human. Of course, there
should be a cutoff, and the sheer amount of applications being submitted means that some narrowing down must occur, but
using machine learning by adapting the algorithm to show candidates most similar to the already hired would not be a good
unbiased approach. This is especially dangerous because unlike with human review, applicants are being biased against in
a relatively invisible way. As it says in the article, these are "old techniques dressed up in new technology." Since
the algorithm's designers failed to account for the historical trends in the training data, bias still exists despite
the fact the algorithm is doing what it's supposed to do.


2.
data/train_candidates_unequal.csv
- Female 0.12 | Male 0.22
- Female 0.09 | Male 0.35
- Female 0.13 | Male 0.23
- Female 0.14 | Male 0.36
- Female 0.08 | Male 0.18

I noticed that the female percentages are always lower than the male percentages. While the female percentages seem to
never go above 0.15, the male percentages stay above 0.15, sometimes significantly so. In fact, the males seem to get hired
around twice as likely as their female counterparts.

3.
data/train_candidates_equal.csv
- Female 0.11 | Male 0.24
- Female 0.11 | Male 0.17
- Female 0.12 | Male 0.23
- Female 0.11 | Male 0.25
- Female 0.1 | Male 0.19

There is still bias in the algorithm. However, in comparison to the other filepath, it isn't more equal.
Even though, the male's get hired more percentage wise, the female hire rate seems to be at an increase often to the male
ratio. However, it does still have the lower percentage while running it a few times. There are some bias still.
A male got in with a lesser gpa while it seems like women cannot be admitted unless their gpa would be higher than 3.5.
Values like these are the constant bias that would occur even though the hiring practices seem equal.

4. The approach we took in our code would barely be impacting the resulting bias because our code chooses
random attributes to split on after the attribute that we target on. This would have some bias as it would
have a bias on what the company would prefer on.

5. The reason why the hiring rates change everytime we build a new classifier is because it randomly chooses which
attribute matters to the hiring company. So, if the company values leadership experience first then GPA, then those without
leadership experience even be considered for GPA. These types of examples would show how biased our code is when considering
various attributes, which in turn changes the hiring rates. If we choose the same attribute to split on each time,
the bias would not be eliminated because it would be biased toward the people who have those specific qualifications.

6. There is still some evidence of hiring bias because the percentage of male and females are still discrepant.
The male percentages are still consistently higher than the female percentages, even though it is still closer than the
other filepaths. This descreptancy indicates that leadership experience and last position duration are correlated with
gender.

7. The limitations of BiasTest is that it would only consider one variable at one time and if the variable
does not match the recommended decision then it would not consider that attribute again since we cannot
access that attribute again. So, if the candidate would have some factor that does not work then it would
not consider the candidate again. Thus, if there would be a factor of gender, then all females could be considered
while males would not be considered at all. There are a lot of external factors for a candidate but using this system
would completely eliminate them because it would not consider them at all.
Another reason why it is limiting is that these algorithms should not neglect
important questions of "who benefits and suffers" instead of directing the problem to "diversity and inclusion"
I do not believe that there are any strengths. One close benefit would be parsing through a dataset to directly
find candidates that would find what specifically what a company would be searching for. One example that it
would not capture would be analyzing more accurate diagnostic tools. This is because our code would determine
the most common value but if the most common value would be wrong then our code would return a value that would
not be as accurate as compared to in the article where their machine learning could predict up to "97% accuracy".





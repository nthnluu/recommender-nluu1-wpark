
Group Members : William Park, Ethan Kim, Nathan Luu
Subgroup 1 : William Park, Nathan Luu
Subgroup 2 : Ethan Kim

Our code uses machine learning that would produce a decision tree to summarize what we prefer.
For our tests, we used the vegetable examples. The decisions tree would have a node that does not have a leaf has
an attribute, the same attribute arises once, the node has edges, and there will be the predicated value for one
attribute. Our ListObjsData class would hold our dataset where we would have methods to use the dataset. We have
attributes to get the attributes (getAttribute), get the same value (allSameValue), get the size (size), split the
dataset into two subsets based on the attribute that we decided on, etc. This class would be used to produce the
dataset that would be used in the TreeGenerator class. The TreeGenerator class would generate a tree. The TreeGenerator
class would use a Node and Leaf classes in order to generate the tree.

1. The narrowing of the application pool shows bias as there will be  a strong focus on the strongest of candidates.
This is important to know because once an employer hires an application the algorithm that they use would use this pick
to show other candidates of similar qualifications. This would show favoritism to candidates and not give a chance to
other candidates and not considering the other candidates at all. Of course, there should be a cutoff but using machine
 learning by adapting the algorithm to show candidates most similar to the already hired would not be a good unbiased
 opinions. This is interesting as it says in the the article that these are "old techniques dressed up in new
 technology". Bias still exists even though it has turned into a code. Another thought that I have is the equality
  initiative. Even though this is an act to hire more diverse peoples, I do think it has some bias towards the people
  who are not as qualified as other people.


2.
data/train_candidates_unequal.csv
- Female 0.03| Male 0.3
- Female 0.04| Male 0.14
- Female 0.015| Male 0.23
- Female 0.09| Male 0.26
- Female 0.08| Male 0.18

I notice that the female percentages are always lower than the male percentages.
The female percentages seem to never go about 0.10 while the male percentages stay about 0.10.

3.
data/train_candidates_equal.csv
- Female 0.15| Male 0.28
- Female 0.4| Male 0.13
- Female 0.10| Male 0.17
- Female 0.14| Male 0.20
- Female 0.10| Male 0.21

There is still bias in the algorithm. However, in comparison to the other filepath, it is more equal.
Even though, the male's get hired more percentage wise, the female hire rate increased enough to be close to the male
ratio. Even in the google sheet the percentage shown was equal. Looking at the google sheet, there are some bias still.
A male got in with a lesser gpa while it seems like women cannot be admitted unless their gpa would be higher than 3.5.
Values like these are the constant bias that would occur even though the hiring practices seem equal.

4. The approach we took in our code would barely be impacting the resulting bias because our code chooses
random attributes to split on after the attribute that we target on. This would have some bias as it would
have a bias on what the company would prefer on.

5. The reason why the hiring rates change everytime is because it would choose which attribute matters to the hiring
company. So, if the company values leadership experience first then GPA, then those without the leadership experience
wouldn't be able to be hired than those with leadership experience. These types of examples would show how our code
approaches this problem. This occurs as well because the choosing of random attributes after the target would change
the rates. If we choose the same attribute to split on each time, the bias would not be eliminated because it would
 be bias toward the people who have those qualifications.

6. There is still some evidence of bias because the percentage of male and females are still discrepant.
The female percentage is still lower even though it is still closer than the other filepaths.

7. The limitations of BiasTest is that it would only consider one variable at one time and if the variable
does not match the recommended decision then it would not consider that attribute again since we cannot
access that attribute again. So, if the candidate would have some factor that does not work then it would
not consider the candidate again. Thus, if there would be a factor of gender, then all females could be considered
while males would not be considered at all. There are a lot of external factors for a candidate but using this system
would completely eliminate them. Another reason why it is limiting is that these algorithms should not neglect
important questions of "who benefits and suffers" instead of directing the problem to "diversity and inclusion"
I do not believe that there are any strengths. One close benefit would be parsing through a dataset to directly
find candidates that would find what specifically what a company would be searching for. One example that it
would not capture would be analyzing more accurate diagnostic tools. This is because our code would determine
the most common value but if the most common value would be wrong then our code would return a value that would
not be as accurate as compared to in the article where their machine learning could predict up to "97% accuracy".




